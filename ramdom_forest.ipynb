'''
Stock Factors & Random Forest
MF850 GroupProj

Silvia
'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#%% DATA PREPROCESSING

# df_noret = pd.read_csv("final_training.csv") # prediction df
df = pd.read_csv("final_training_xy.csv") # train

#%% Data Type Check and Dropping Date Columns

# Investigation in data types
print(df.dtypes.value_counts())

# Delete object type columns except date column
object_cols = df.dtypes[df.dtypes == 'object'].index.tolist()
object_cols.remove('date')
object_cols += ['fqtr'] #'dvpspq', 'dvpsxq', 'm_divs'

df = df.drop(object_cols, axis=1) # drop object type columns (all dates)

print("\n%d date columns dropped"%len(object_cols))

#%% NA Values Check and Initial Handling

# NAIVE HANDLING: DROP ALL COLUMNS CONTAIN NA
# na_cols = na_data.index.tolist()
# df = df.drop(na_cols, axis=1)

# Make sure y has no NA
df = df[df['m_ret_next'].notna()].copy()
print("\nNA rows for m_ret_next are dropped.")

na_data = df.isna().sum()[df.isna().sum() > 0]
print(na_data) # Mostly accounting data have NAs

## Drop columns with more than 10000 missing NAs
df = df.drop(na_data[na_data > 10000].index.tolist(), axis=1)
print("\n%d columns dropped due to more than 10000 NAs"%(na_data > 10000).sum())

#%% Check GIC attributes 

gic = df.groupby(['comp_id', 'ggroup', 'gind', 'gsector', 'gsubind']).size().reset_index().rename(columns={0:'count'})
gic_count = gic.groupby('comp_id').count().ggroup
na_gic_compid = set(df.comp_id.unique()) - set(gic.comp_id)
print("\n# companies not having unique set of GIC attributes:", len(gic_count[gic_count != 1]))
print("\n# total companies: %d \
      \n# companies have GIC: %d \
      \nCompany with NA GIC: %s" 
      %(len(df.comp_id.unique()), len(gic), na_gic_compid))

## Eliminate the company doesn't have full GIC attributes
df = df[df[['ggroup', 'gind', 'gsector', 'gsubind']].notna().all(axis=1)].copy()
print("\n%s company is eliminated due to missing GIC attributes"%na_gic_compid)

#%% Drop columns that at least one company has all NAs

# Aggregate total NA columns for each company
def get_allna_col(cdf):
    col_na_count = cdf.isna().sum()
    cols = col_na_count[col_na_count == len(cdf)].index.tolist()
    return cols

allna = df.groupby('comp_id').apply(get_allna_col)

# attributes that are all NA for at least one company
allna_colnames = set(sum(allna.values.tolist(), []))

# drop the columns
df = df.drop(allna_colnames, axis=1)
print("\n%d columns dropped since at least one company has all NAs"%(len(allna_colnames)))

#%% Check remaining NA and Ffill&Bfill accounting data

na_comp = pd.Series(index=gic.comp_id, 
                    data=gic.comp_id.map(lambda x: df[df.comp_id == x].isna().sum().sum()).values)
na_compids = na_comp[na_comp > 0].index.tolist()
good_compids = list(set(df.comp_id.unique()) - set(na_compids))

# Ffill then Bfill accounting data for each company that still has NA
dfs = []
totalrow = 0
for cid in na_compids:

    cdf = df[df.comp_id == cid].copy()
    new_cdf = cdf.sort_values('date').fillna(method='ffill').fillna(method='bfill').copy()
    dfs.append(new_cdf)
    
    # if new_cdf.isna().sum().sum() > 0:
    #     print(cid, "has missing beginning data, dropped")
    #     print(new_cdf[new_cdf.isna().sum()[new_cdf.isna().sum() > 0].index.tolist()])
    # else:
    #     dfs.append(new_cdf)
    #     totalrow += len(new_cdf)
        
togo_df = pd.concat(dfs, axis=0) # concat each new dfs
df = df[df.comp_id.isin(good_compids)].append(togo_df).sort_index() # combine


#%% Trading Indicators

# Add trading indicators which do not require lag

# Close Location Value https://www.investopedia.com/terms/c/close_location_value.asp
df['CLV'] = ((2*df.close - df.m_high - df.m_low)/(df.m_high - df.m_low)).fillna(0)
df['CLV_adj'] = ((2*df.close_adj - df.m_high_adj - df.m_low_adj)/(df.m_high_adj - df.m_low_adj)).fillna(0)

# Money Flow Volumn https://www.investopedia.com/terms/a/accumulationdistribution.asp
df['MFV'] = df.CLV * df.m_volume
df['MFV_adj'] = df.CLV_adj * df.m_volume_adj

df['Range'] = df.m_high - df.m_low
df['Range_adj'] = df.m_high_adj - df.m_low_adj

#%% RANDOM FOREST

from sklearn.model_selection import train_test_split

# For RF, X variable columns needs to exclude date and y
x_cols = df.columns.tolist()
x_cols.remove('date')
x_cols.remove('m_ret_next')

X = df[x_cols].copy()
y = np.where(df.m_ret_next < 0, 0, 1) # negative return as 0, positive return as 1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#%% ML HELPERS
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, f1_score, accuracy_score

def result(y,pred,name):

  print(30*'-' + name + 30*'-')

  cm = confusion_matrix(y, pred)
  accuracy = accuracy_score(y, pred)*100
  F1_score = f1_score(y,pred)*100

  print('Confusion matrix:\n', cm)
  print('Accuracy: %.4f %%' % accuracy)
  # print('AUC: %.4f %%' % auc_roc)
  print('f1 score: %.4f %%' % F1_score)

  # summary report
  cs = classification_report(y, pred, digits=6)
  print('\nClassification Summary:\n', cs)

  return {name: {'Accuracy': accuracy, 'F1-score': F1_score}}

#%% Defatul Parameter
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)
y_pred_base = rfc.predict(X_test)
base_result = result(y_test, y_pred_base, "Default Params")

#%% Choose good ccp_alpha by decision tree
import ccp_alpha_chooser

ccp_alpha_chooser.evaluate(X_train, X_test, y_train, y_test)

#%% Random search CV to find a good structure

from sklearn.model_selection import GridSearchCV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)]

criterion = ['entropy']

# Number of features to consider at every split
max_features = ['sqrt', 'log2']

# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 6)]
max_depth.append(None)

# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]

# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]

# Whether bootstrap samples are used when building trees. 
# If False, the whole dataset is used to build each tree.
bootstrap = [True]

oob_score = [True, False]

ccp_alpha = np.linspace(0.00002, 0.000008, num = 6)

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'criterion': criterion,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

print(random_grid)


#%%
# Grid search of parameters, using 3 fold cross validation
rfc_cv = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid,
                              scoring='accuracy', 
                              cv = 3, verbose=2, n_jobs=3,
                              return_train_score=True)

# Fit the random search model
rfc_cv.fit(X_train, y_train);

#%%

y_pred = rfc_random.predict(X_test)

rf_result = result(y_test, y_pred, "Random CV")

#%%

def drop_via_corr(X, threshold = 0.7):\
  corr = X.corr()
  high_corr = (corr.abs() > threshold).sum()
  print((high_corr > 1).sum(), "features have stronge correlation with at least one of the other feature.")
  high_corr = corr[high_corr[high_corr>1].index].loc[high_corr[high_corr>1].index]

  features = high_corr.columns.tolist()
  chosen_features = []
  i = 0
  while features:
      
      cur_feature = high_corr.columns[i]
      if (cur_feature not in chosen_features) and (cur_feature in features):
          chosen_features.append(cur_feature)
          features.remove(cur_feature)
      
      col = high_corr.iloc[(i+1):, i].copy()
      hc = col[col.abs() > 0.7].index.tolist()
      
      for h in hc:
          if h in features:
              features.remove(h)
      i+=1
    
  print(len(chosen_features), 'features chosen.')

  drop_features = list(set(high_corr.columns.tolist()) - set(chosen_features))
  X = X.drop(columns=drop_features, axis=1)
  print('Number of features to be used:', X.shape[1])

  return X

#%%

# X = drop_via_corr(df_noret.drop(['date', 'comp_id'], axis=1))
# X_objects = X.dtypes[X.dtypes == 'object'].index.tolist()
# X = X.drop(X_objects, axis=1)
# y = 

#%%

# df.groupby('comp_id').count().iloc[:, 0].describe()

#%%

# df['high_over_low'] = df.m_high / df.m_low

#%%
# df
